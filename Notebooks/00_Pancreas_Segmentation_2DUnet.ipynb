{"cells":[{"cell_type":"markdown","metadata":{"id":"i3J77vMBckol"},"source":["# Pancreas Segmentation\n","2D-UNET approach.\n","Data are supposed to be on Google Drive\n","\n","## Test Setup\n","- Image resolution: 128x128x128\n","- Network architecture: 8-16-32-64-32-16-8\n","- Constant Learning Rate\n"]},{"cell_type":"markdown","metadata":{"id":"LzHkeBPJcxXV"},"source":["## Environment Setup\n","Here we setup the environment. This is a required step to get access to data from Google Drive."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":2251,"status":"ok","timestamp":1595320563275,"user":{"displayName":"Neuro Engineer","photoUrl":"","userId":"17805442247735853420"},"user_tz":-120},"id":"rxYpV9-jcw6g","outputId":"cad19df3-2b11-4147-c33c-bf6837da12a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["# SETUP FOR DRIVE ENVIRONMENT\n","from google.colab import drive\n","ROOT_PATH = '/content/gdrive'\n","drive.mount(ROOT_PATH, force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"NvSIzZpldRzQ"},"source":["## Python Modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X2TXYN97dToh"},"outputs":[],"source":["# MODULES TO BE IMPORTED\n","from datetime import datetime\n","from matplotlib import pyplot as plt\n","import math\n","import nibabel as nib\n","import numpy as np\n","import os\n","import pandas as pd\n","import pickle\n","import re\n","import seaborn as sns\n","from sklearn.utils import shuffle\n","import sys\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","import time"]},{"cell_type":"markdown","metadata":{"id":"vu9oqPC7drBX"},"source":["### Constants\n","Here you can change the path where data are loaded from, and where the model is saved. You should change the variable DATA_PATH.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C1GNFGcZdrI7"},"outputs":[],"source":["# FOLDER TO LOAD DATA FROM\n","MAIN_PATH = os.path.join(ROOT_PATH, 'My Drive', 'Colab Notebooks')\n","CODE_PATH = os.path.join(MAIN_PATH, 'PANCREAS_2.5D', 'Notebooks')\n","DATA_PATH = os.path.join(MAIN_PATH, 'PANCREAS_2.5D', 'Data', 'Training')\n","MODEL_PATH = os.path.join(MAIN_PATH, 'PANCREAS_2.5D', 'Model')\n","\n","# SETUP\n","# Volume sizeTest\n","N_ROWS = 128\n","N_COLUMNS = 128\n","N_SLICES = 128\n","\n","# Type\n","SHAPE_OF_INTEREST = 'pancreas'\n","VOLUME_TYPE = 'nii'  # double\n","VOLUME_TEMPLATE = (\"{path}/Patient{patient}/volumeCT_reshape_{shape}_\"\n","                   \"{rows}_{columns}_{slices}_{patient}.{volume}\")\n","\n","LABEL_TEMPLATE = (\"{path}/Patient{patient}/volumeLabel_reshape_{shape}_\"\n","                  \"{rows}_{columns}_{slices}_{patient}.{volume}\")\n","########################\n","#     SEGMENTATION     #\n","########################\n","N_CLASSES = 3\n","'''\n","C0: background      (value 0 in the original label volume)\n","C1: 'pancreas'      (value 1 in the original label volume)\n","C2: pancreas lesion (value 2 in the original label volume)\n","'''\n","\n","########################\n","#         DATA         #\n","########################\n","# TRAINING PERCENTAGE\n","TRAINING_PERC_CASES = 0.80\n","# VALIDATION\n","VALIDATION_PERC_CASES = 0.10\n","# PERCENTAGE\n","TEST_PERC_CASES = 1 - TRAINING_PERC_CASES - VALIDATION_PERC_CASES\n","\n","########################\n","#         MODEL        #\n","########################\n","# Concatenation\n","CONCATENATION_DIRECTION_OF_FEATURES = 3\n","# OPTIMIZER\n","# Regularization factor lambda\n","L2_REG_LAMBDA = 0.001\n","# Maximum nuber of epochs\n","MAX_EPOCHS = 200\n","'''\n","TAU factor in the learning rate function\n","(INITIAL_LEARNING_RATE - FINAL_LEARNING_RATE) * 1 / (1 + math.exp((epoch_number - MAX_EPOCHS) / TAU_EPOCHS))) + FINAL_LEARNING_RATE)\n","'''\n","TAU_EPOCHS = 25\n","# Size for batch normalization\n","BATCH_SIZE = 10"]},{"cell_type":"markdown","metadata":{"id":"uXVfm_jTeEJP"},"source":["## Split dataset in training, validation, and test\n","Here we split the dataset file paths into train, validation,and test sets according to the specified percentages. This is required so that the two generators can get the correct data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J5yvUXv4eEPu"},"outputs":[],"source":["volumes_list = []  # List of volume file names\n","labels_list = []  # List of label file names\n","\n","total_number_of_cases = 0\n","# For loop over all the folders in data path\n","for patient_index, patient_folder in enumerate(os.listdir(DATA_PATH)):\n","    case_id = \"{:0>3}\".format(patient_index + 1)\n","    # Set up volume and label path\n","    volume_path = VOLUME_TEMPLATE.format(\n","        path=DATA_PATH,\n","        patient=case_id,\n","        shape=SHAPE_OF_INTEREST,\n","        rows=N_ROWS,\n","        columns=N_COLUMNS,\n","        slices=N_SLICES,\n","        volume=VOLUME_TYPE\n","    )\n","    label_path = LABEL_TEMPLATE.format(\n","        path=DATA_PATH,\n","        patient=case_id,\n","        shape=SHAPE_OF_INTEREST,\n","        rows=N_ROWS,\n","        columns=N_COLUMNS,\n","        slices=N_SLICES,\n","        volume=VOLUME_TYPE\n","    )\n","    # Check if both paths exist\n","    if os.path.exists(volume_path) and os.path.exists(label_path):\n","        total_number_of_cases += 1\n","        volumes_list.append(volume_path)\n","        labels_list.append(label_path)\n","\n","print('Total number of cases: {}'.format(total_number_of_cases))\n","\n","# Get train and test set\n","# Get percentages for splitting\n","training_index = int(total_number_of_cases * TRAINING_PERC_CASES)\n","validation_index = training_index + \\\n","    int(total_number_of_cases * VALIDATION_PERC_CASES)\n","test_index = validation_index + \\\n","    int(total_number_of_cases*(1-TRAINING_PERC_CASES-VALIDATION_PERC_CASES))\n","\n","# Split in train, validation, and test\n","train_volumes_list = volumes_list[0:training_index]\n","train_labels_list = labels_list[0:training_index]\n","validation_volumes_list = volumes_list[training_index:validation_index]\n","validation_labels_list = labels_list[training_index:validation_index]\n","test_volumes_list = volumes_list[validation_index:test_index]\n","test_labels_list = labels_list[validation_index:test_index]\n","\n","print('Number of training cases: {}'.format(len(train_volumes_list)))\n","print('Number of validation cases: {}'.format(len(validation_volumes_list)))"]},{"cell_type":"markdown","metadata":{"id":"V3nbwkD8d4AG"},"source":["## Data generator\n","This is the generator that loads data for the model. The generator is used to load only the required samples for the current batch of the network, thus reducing the total RAM required."]},{"cell_type":"markdown","metadata":{"id":"1EsmO63dd7k5"},"source":["### Class for DataGenerator\n","This is the class used for the data generator."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dYAZleGEd8LP"},"outputs":[],"source":["class DataGenerator(keras.utils.Sequence):\n","    '''\n","    Class used for data generators.\n","    Reference:\n","    '''\n","\n","    def __init__(self, id_list, batch_size=10, dim=(200, 200), n_slices=128, shuffle=True):\n","        '''\n","        Function called when initializing the class.\n","        '''\n","        self.id_list = id_list\n","        for patient_counter, patient_index in enumerate(self.id_list):\n","            for slice_number in range(n_slices):\n","                if (slice_number == 0):\n","                    patient_list = np.array([[patient_index, slice_number]])\n","                else:\n","                    patient_list = np.append(\n","                        patient_list, [[patient_index, slice_number]], axis=0)\n","            if (patient_counter == 0):\n","                self.slices_list = patient_list\n","            else:\n","                self.slices_list = np.append(\n","                    self.slices_list, patient_list, axis=0)\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","        self.n_slices = n_slices\n","        self.dim = dim\n","        self.on_epoch_end()\n","\n","    def on_epoch_end(self):\n","        '''\n","        Updates indexes after each epoch. If shuffle is set\n","        to True, the indexes are shuffled. Shuffling the order in which examples\n","        are fed to the classifier is helpful so that batches between\n","        epochs do not look alike. Doing so will eventually make our model more robust.\n","        '''\n","        self.indexes = np.arange(len(self.slices_list))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)\n","\n","    def __data_generation(self, list_IDs_temp):\n","        '''\n","        Generates data containing batch_size samples\n","        X : (n_samples, *dim, n_channels)\n","        '''\n","        # Initialization\n","        X = np.empty((self.batch_size, *self.dim))\n","        Y = np.empty((self.batch_size, *self.dim))\n","\n","        # Generate data\n","        for index, ID in enumerate(list_IDs_temp):\n","            # Store volume\n","            temp_volume = nib.load(ID[0][0])\n","            temp_volume = temp_volume.get_fdata()\n","            temp_volume = np.asarray(temp_volume)\n","            X[index, :, :] = temp_volume[:, :, ID[1]]\n","            # Store label\n","            temp_label = nib.load(ID[0][1])\n","            temp_label = temp_label.get_fdata()\n","            temp_label = np.asarray(temp_label)\n","            Y[index, :, :] = temp_label[:, :, ID[1]]\n","\n","        X = X.reshape(X.shape + (1,))  # necessary to give it as input to model\n","        Y = self.remap_labels(Y)\n","        return X, Y\n","\n","    # Map 3D to 4D labels\n","    def remap_labels(self, labels_3D):\n","        labels_4D = np.zeros(labels_3D.shape + (N_CLASSES, ))\n","        # Scan the classes\n","        for c in range(N_CLASSES):\n","            temp_indexes = np.where(labels_3D == c)\n","            labels_4D[temp_indexes +\n","                      (np.ones(temp_indexes[0].shape, dtype='int') * c, )] = 1\n","        return labels_4D\n","\n","    def __len__(self):\n","        'Denotes the number of batches per epoch'\n","        return int(np.floor(len(self.slices_list) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        # Generate indexes of the batch\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","        # Find list of IDs\n","        id_list_temp = [self.slices_list[k] for k in indexes]\n","\n","        # Generate data\n","        X, y = self.__data_generation(id_list_temp)\n","\n","        return X, y"]},{"cell_type":"markdown","metadata":{"id":"P17fOl-UemxO"},"source":["## Custom Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":2746,"status":"ok","timestamp":1595320563887,"user":{"displayName":"Neuro Engineer","photoUrl":"","userId":"17805442247735853420"},"user_tz":-120},"id":"zcPhpLjVem8f","outputId":"bbeee685-be19-44e0-d3cb-a0915a575341"},"outputs":[{"name":"stdout","output_type":"stream","text":["[<function IouMetricFactory.<locals>.fn at 0x7f2abcce8158>, <function IouMetricFactory.<locals>.fn at 0x7f2abccccea0>, <function IouMetricFactory.<locals>.fn at 0x7f2abccccf28>]\n"]}],"source":["def IoUMetricFunction(y_true, y_pred, class_value):\n","    '''\n","    Compute metric IoU for parameter y_true and y_pred only for the\n","    specified class.\n","\n","    Input y_true and y_pred is supposed to be 5-dimensional:\n","    (batch, x, y, softmax_probabilities)\n","    '''\n","    class_IoU_list = []\n","    y_true = tf.cast(y_true, 'bool')\n","    # argmax to choose which class the model predicts for each voxel\n","    y_pred = tf.argmax(y_pred, axis=-1, output_type='int64')\n","    y_pred = tf.one_hot(indices=y_pred, depth=N_CLASSES,\n","                        axis=-1, dtype='int64')\n","    y_pred = tf.cast(y_pred, 'bool')\n","    y_true_c = y_true[:, :, :, class_value]\n","    y_pred_c = y_pred[:, :, :, class_value]\n","    tp = tf.math.count_nonzero(tf.logical_and(y_true_c, y_pred_c))\n","    fn = tf.math.count_nonzero(tf.logical_and(\n","        tf.math.logical_xor(y_true_c, y_pred_c), y_true_c))\n","    fp = tf.math.count_nonzero(tf.logical_and(\n","        tf.math.logical_xor(y_true_c, y_pred_c), y_pred_c))\n","    # single scalar, already averaged over different instances\n","    return tp / (tp + fn + fp)\n","\n","\n","def IouMetricFactory(class_value):\n","    '''\n","    This function is only used to assign a name to the given IoU metric.\n","    '''\n","    def fn(y_true, y_pred):\n","        return IoUMetricFunction(y_true, y_pred, class_value)\n","\n","    fn.__name__ = 'class_{}_IoU'.format(class_value)\n","    return fn\n","\n","\n","# Create a list of functions, so that they can be used in model training.\n","my_metrics = []\n","for c in range(0, N_CLASSES):\n","    my_metrics.append(IouMetricFactory(c))\n","\n","print(my_metrics)"]},{"cell_type":"markdown","metadata":{"id":"i8TbxCRxesXg"},"source":["## Model Setup\n","Here, we setup the model. We define the model with custom callbacks, architecture, and so on."]},{"cell_type":"markdown","metadata":{"id":"_x0-JPtpestb"},"source":["### Model callbacks\n","- **early stopping and time monitoring**: this callback checks the training loss, and it its value is lower than 0.001, it stops the training process. It also prints out time information, so that we can find out how longer the training process is.\n","- **validation early stopping**: this callback checks if, for three consecutive times, the IoU metric on class 1 and class 2 is greater than 95%. If this is true, then the training is stopped.\n","- **learning rate**: this is a learning rate scheduler that allows to set up a schedule for the learning rate.\n","-**metrics plot**: this callback plots the accuracy and loss on both the training and validation set in a figure that is stored in the model directory."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wd5NJLDReujD"},"outputs":[],"source":["import seaborn as sns\n","# Early stopping and time monitoring callback\n","\n","\n","class EarlyStoppingAndInfo(tf.keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs={}):\n","        global tic\n","        toc = datetime.now()\n","        seconds_training = (toc-tic).total_seconds()\n","        hours, remainder = divmod(seconds_training, 3600)\n","        minutes, seconds = divmod(remainder, 60)\n","        print('\\n{:02d}:{:02d}:{:02d} spent for training'.format(\n","            math.floor(hours), math.floor(minutes), math.floor(seconds)))\n","        if logs['loss'] < 0.001:\n","            self.model.stop_training = True\n","            print('\\nNOTE: out of training because overfitting')\n","\n","\n","class ValidationEarlyStopping(tf.keras.callbacks.Callback):\n","\n","    def on_train_begin(self, logs={}):\n","        # Init variables\n","        self.epoch_counter = 0    # Epoch counter\n","        self.val_class_1_IoU = []  # Class 1 IoU\n","        self.val_class_2_IoU = []  # Class 2 IoU\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        self.val_class_1_IoU.append(logs.get('val_class_1_IoU'))\n","        self.val_class_2_IoU.append(logs.get('val_class_2_IoU'))\n","        # Check if last three elements are greater than 0.95 for both classes\n","        class_1_check = (\n","            (np.asarray(self.val_class_1_IoU[-3:]) >= 0.95).sum() == 3)\n","        class_2_check = (\n","            (np.asarray(self.val_class_2_IoU[-3:]) >= 0.95).sum() == 3)\n","        if (class_1_check and class_2_check and self.epoch_counter >= 50):\n","            self.model.stop_training = True\n","            print('\\nNOTE: IoU greater than 95% for 5 consecutive epochs.')\n","\n","\n","earlyStopperAndInfo = EarlyStoppingAndInfo()\n","\n","# Learning rate program:\n","\n","\n","def my_learning_rate_program(epoch_number=0, current_learning_rate=0):\n","    \"\"\" 'current_learning_rate' argument not used, just for the sake of completeness \"\"\"\n","    return ((INITIAL_LEARNING_RATE - FINAL_LEARNING_RATE) * 1 / (1 + math.exp((epoch_number - MAX_EPOCHS) / TAU_EPOCHS))) + FINAL_LEARNING_RATE\n","\n","\n","learningRateScheduler = tf.keras.callbacks.LearningRateScheduler(\n","    my_learning_rate_program, verbose=1)\n","\n","# plt.plot(range(0, MAX_EPOCHS), [my_learning_rate_program(epoch_number = n) for n in range(0, MAX_EPOCHS)])\n","# plt.title('learning rate with epochs:')\n","# plt.show()\n","\n","# Plot accuracy and loss\n","\n","\n","class PlotMetrics(tf.keras.callbacks.Callback):\n","\n","    def __init__(self, **kwargs):\n","        self.previous_data = False\n","        self.epoch_counter = 0\n","        super()\n","\n","    def set_folder(self, folder):\n","        self.folder = folder\n","\n","    def load_data(self, folder):\n","        self.previous_data = True\n","        self.data = pd.read_csv(os.path.join(\n","            MODEL_PATH, folder, 'TrainingMonitoring.csv'))\n","\n","    def on_train_begin(self, logs={}):\n","        if (not self.previous_data):\n","            # Init all arrays\n","            self.data = pd.DataFrame(columns=['Tr Loss',\n","                                              'Cl0 IoU',\n","                                              'Cl1 IoU',\n","                                              'Cl2 IoU',\n","                                              'Val Loss',\n","                                              'VCl0 IoU',\n","                                              'VCl1 IoU',\n","                                              'VCl2 IoU'],\n","                                     dtype=float)\n","\n","    def on_epoch_end(self, batch, logs={}):\n","        # Append metrics\n","        self.data.loc[self.epoch_counter, 'Tr Loss'] = float(logs.get('loss'))\n","        self.data.loc[self.epoch_counter, 'Cl0 IoU'] = float(\n","            logs.get('class_0_IoU'))\n","        self.data.loc[self.epoch_counter, 'Cl1 IoU'] = float(\n","            logs.get('class_1_IoU'))\n","        self.data.loc[self.epoch_counter, 'Cl2 IoU'] = float(\n","            logs.get('class_2_IoU'))\n","        self.data.loc[self.epoch_counter, 'Val Loss'] = float(\n","            logs.get('val_loss'))\n","        self.data.loc[self.epoch_counter, 'VCl0 IoU'] = float(\n","            logs.get('val_class_0_IoU'))\n","        self.data.loc[self.epoch_counter, 'VCl1 IoU'] = float(\n","            logs.get('val_class_1_IoU'))\n","        self.data.loc[self.epoch_counter, 'VCl2 IoU'] = float(\n","            logs.get('val_class_2_IoU'))\n","        self.epoch_counter += 1\n","\n","        # print(self.data)\n","        # .. And plot them!\n","\n","        plt.figure(figsize=(10, 10))\n","        sns.set(style='darkgrid')\n","\n","        ax = sns.lineplot(x=range(0, len(self.data)), y='Tr Loss', data=self.data, ci=None,\n","                          label='Tr Loss', linewidth=3)\n","        sns.lineplot(x=range(0, len(self.data)), y='Cl0 IoU', data=self.data, ci=None,\n","                     label='0 IoU', linewidth=3, ax=ax)\n","        sns.lineplot(x=range(0, len(self.data)), y='Cl1 IoU', data=self.data, ci=None,\n","                     label='1 IoU', linewidth=3, ax=ax)\n","        sns.lineplot(x=range(0, len(self.data)), y='Cl2 IoU', data=self.data, ci=None,\n","                     label='2 IoU', linewidth=3, ax=ax)\n","        sns.lineplot(x=range(0, len(self.data)), y='Val Loss', data=self.data, ci=None,\n","                     label='V Loss', linewidth=3, ax=ax)\n","        sns.lineplot(x=range(0, len(self.data)), y='VCl0 IoU', data=self.data, ci=None,\n","                     label='V0 IoU', linewidth=3, ax=ax)\n","        sns.lineplot(x=range(0, len(self.data)), y='VCl1 IoU', data=self.data, ci=None,\n","                     label='V1 IoU', linewidth=3, ax=ax)\n","        sns.lineplot(x=range(0, len(self.data)), y='VCl2 IoU', data=self.data, ci=None,\n","                     label='V2 IoU', linewidth=3, ax=ax)\n","        ax.set_xlabel(\"Epochs\", fontsize=16)\n","        ax.set_ylabel(\"\", fontsize=16)\n","        ax.set_title('Training Monitoring', fontsize=20)\n","        # Save Figure\n","        fig = ax.get_figure()\n","        fig.savefig(os.path.join(\n","            MODEL_PATH, self.folder, 'TrainingMonitoring.png'))\n","        # Save dataframe\n","        self.data.to_csv(os.path.join(MODEL_PATH, self.folder,\n","                         'TrainingMonitoring.csv'), index=False)\n","        plt.close()\n","\n","\n","plotMetrics = PlotMetrics()\n","\n","# my_callbacks_list = [earlyStopperAndInfo, learningRateScheduler, plotMetrics]\n","my_callbacks_list = [earlyStopperAndInfo, plotMetrics]"]},{"cell_type":"markdown","metadata":{"id":"BEEZP8n3ew-w"},"source":["### Model architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":12231,"status":"ok","timestamp":1595320573398,"user":{"displayName":"Neuro Engineer","photoUrl":"","userId":"17805442247735853420"},"user_tz":-120},"id":"vFqVTq_7exEi","outputId":"87ea505f-8470-4cf7-8816-653c2a8a38ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["Parameters in model: 481923\n"]}],"source":["# Network Model\n","\n","# Initialize input size\n","my_input_tensor = tf.keras.layers.Input(\n","    shape=(N_ROWS, N_COLUMNS, 1))  # , dtype = 'float16'\n","\n","# Here we define some parameters of the networks\n","\n","# First processing layer\n","if True:\n","    # Convolution 16 filters\n","    cumulative_resulting_tensor = tf.keras.layers.Conv2D(16, kernel_size=(3, 3),\n","                                                         strides=(1, 1),\n","                                                         padding='same',\n","                                                         activation='linear',\n","                                                         kernel_regularizer=tf.keras.regularizers.l2(\n","                                                             L2_REG_LAMBDA),\n","                                                         bias_regularizer=tf.keras.regularizers.l2(L2_REG_LAMBDA))(my_input_tensor)\n","    # Batch for averaging\n","    cumulative_resulting_tensor = tf.keras.layers.BatchNormalization(\n","        axis=-1)(cumulative_resulting_tensor)\n","    # RELU\n","    cumulative_resulting_tensor = tf.keras.layers.Activation(\n","        'relu')(cumulative_resulting_tensor)\n","    # Convolution 16 filters\n","    cumulative_resulting_tensor = tf.keras.layers.Conv2D(16, kernel_size=(3, 3),\n","                                                         strides=(1, 1),\n","                                                         padding='same',\n","                                                         activation='linear',\n","                                                         kernel_regularizer=tf.keras.regularizers.l2(\n","                                                             L2_REG_LAMBDA),\n","                                                         bias_regularizer=tf.keras.regularizers.l2(L2_REG_LAMBDA))(cumulative_resulting_tensor)\n","    # Batch for averaging\n","    cumulative_resulting_tensor = tf.keras.layers.BatchNormalization(\n","        axis=-1)(cumulative_resulting_tensor)\n","    # RELU\n","    cumulative_resulting_tensor = tf.keras.layers.Activation(\n","        'relu')(cumulative_resulting_tensor)\n","    # Temporary output\n","    intermediate_tensor_1 = cumulative_resulting_tensor\n","    # Maxpooling\n","    cumulative_resulting_tensor = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n","                                                               strides=(2, 2),\n","                                                               padding='same')(cumulative_resulting_tensor)\n","\n","# Second processing layer\n","if True:\n","    # Convolution 32 filters\n","    cumulative_resulting_tensor = tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n","                                                         strides=(1, 1),\n","                                                         padding='same',\n","                                                         activation='linear',\n","                                                         kernel_regularizer=tf.keras.regularizers.l2(\n","                                                             L2_REG_LAMBDA),\n","                                                         bias_regularizer=tf.keras.regularizers.l2(L2_REG_LAMBDA))(cumulative_resulting_tensor)\n","    # Batch for averaging\n","    cumulative_resulting_tensor = tf.keras.layers.BatchNormalization(\n","        axis=-1)(cumulative_resulting_tensor)\n","    # RELU\n","    cumulative_resulting_tensor = tf.keras.layers.Activation(\n","        'relu')(cumulative_resulting_tensor)\n","    # Convolution 32 filters\n","    cumulative_resulting_tensor = tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n","                                                         strides=(1, 1),\n","                                                         padding='same',\n","                                                         activation='linear',\n","                                                         kernel_regularizer=tf.keras.regularizers.l2(\n","                                                             L2_REG_LAMBDA),\n","                                                         bias_regularizer=tf.keras.regularizers.l2(L2_REG_LAMBDA))(cumulative_resulting_tensor)\n","    # Batch for averaging\n","    cumulative_resulting_tensor = tf.keras.layers.BatchNormalization(\n","        axis=-1)(cumulative_resulting_tensor)\n","    # RELU\n","    cumulative_resulting_tensor = tf.keras.layers.Activation(\n","        'relu')(cumulative_resulting_tensor)\n","    # Temporary output\n","    intermediate_tensor_2 = cumulative_resulting_tensor\n","    # Maxpooling\n","    cumulative_resulting_tensor = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n","                                                               strides=(2, 2), padding='same')(cumulative_resulting_tensor)\n","\n","# Third processing layer\n","if True:\n","    # Convolution 64 filters\n","    cumulative_resulting_tensor = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),\n","                                                         strides=(1, 1),\n","                                                         padding='same',\n","                                                         activation='linear',\n","                                                         kernel_regularizer=tf.keras.regularizers.l2(\n","                                                             L2_REG_LAMBDA),\n","                                                         bias_regularizer=tf.keras.regularizers.l2(L2_REG_LAMBDA))(cumulative_resulting_tensor)\n","    # Batch for averaging\n","    cumulative_resulting_tensor = tf.keras.layers.BatchNormalization(\n","        axis=-1)(cumulative_resulting_tensor)\n","    # RELU\n","    cumulative_resulting_tensor = tf.keras.layers.Activation(\n","        'relu')(cumulative_resulting_tensor)\n","    # Convolution 64 filters\n","    cumulative_resulting_tensor = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),\n","                                                         strides=(1, 1),\n","                                                         padding='same',\n","                                                         activation='linear',\n","                                                         kernel_regularizer=tf.keras.regularizers.l2(\n","                                                             L2_REG_LAMBDA),\n","                                                         bias_regularizer=tf.keras.regularizers.l2(L2_REG_LAMBDA))(cumulative_resulting_tensor)\n","    # Batch for averaging\n","    cumulative_resulting_tensor = tf.keras.layers.BatchNormalization(\n","        axis=-1)(cumulative_resulting_tensor)\n","    # RELU\n","    cumulative_resulting_tensor = tf.keras.layers.Activation(\n","        'relu')(cumulative_resulting_tensor)\n","    # Temporary output\n","    intermediate_tensor_3 = cumulative_resulting_tensor\n","    # Maxpooling\n","    cumulative_resulting_tensor = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n","                                                               strides=(2, 2),\n","                                                               padding='same')(cumulative_resulting_tensor)\n","   # Forth processing layer\n","if True:\n","    # Convolution 64 filters\n","    cumulative_resulting_tensor = tf.keras.layers.Conv2D(128, kernel_size=(3, 3),\n","                                                         strides=(1, 1),\n","                                                         padding='same',\n","                                                         activation='linear',\n","                                                         kernel_regularizer=tf.keras.regularizers.l2(\n","                                                             L2_REG_LAMBDA),\n","                                                         bias_regularizer=tf.keras.regularizers.l2(L2_REG_LAMBDA))(cumulative_resulting_tensor)\n","    # Batch for averaging\n","    cumulative_resulting_tensor = tf.keras.layers.BatchNormalization(\n","        axis=-1)(cumulative_resulting_tensor)\n","    # RELU\n","    cumulative_resulting_tensor = tf.keras.layers.Activation(\n","        'relu')(cumulative_resulting_tensor)\n","    # Convolution 64 filters\n","    cumulative_resulting_tensor = tf.keras.layers.Conv2D(128, kernel_size=(3, 3),\n","                                                         strides=(1, 1),\n","                                                         padding='same',\n","                                                         activation='linear',\n","                                                         kernel_regularizer=tf.keras.regularizers.l2(\n","                                                             L2_REG_LAMBDA),\n","                                                         bias_regularizer=tf.keras.regularizers.l2(L2_REG_LAMBDA))(cumulative_resulting_tensor)\n","    # Batch for averaging\n","    cumulative_resulting_tensor = tf.keras.layers.BatchNormalization(\n","        axis=-1)(cumulative_resulting_tensor)\n","    # RELU\n","    cumulative_resulting_tensor = tf.keras.layers.Activation(\n","        'relu')(cumulative_resulting_tensor)\n","    # Temporary output\n","    intermediate_tensor_4 = cumulative_resulting_tensor\n","    # Maxpooling\n","    # cumulative_resulting_tensor = keras.layers.MaxPooling3D(pool_size = (2, 2, 2), strides = (2, 2, 2), padding = 'same')(cumulative_resulting_tensor)\n","\n","# Fourth processing layer\n","if True:\n","    # Deconvolution 32 filters\n","    cumulative_resulting_tensor = tf.keras.layers.Conv2DTranspose(64, kernel_size=(2, 2),\n","                                                                  strides=(\n","                                                                      2, 2),\n","                                                                  padding='same',\n","                                                                  activation='linear',\n","                                                                  kernel_regularizer=tf.keras.regularizers.l2(\n","                                                                      L2_REG_LAMBDA),\n","                                                                  bias_regularizer=tf.keras.regularizers.l2(L2_REG_LAMBDA))(cumulative_resulting_tensor)\n","    # RELU\n","    cumulative_resulting_tensor = tf.keras.layers.Activation(\n","        'relu')(cumulative_resulting_tensor)\n","    # Concatenation\n","    cumulative_resulting_tensor = tf.keras.layers.Concatenate(\n","        axis=CONCATENATION_DIRECTION_OF_FEATURES)([intermediate_tensor_3, cumulative_resulting_tensor])\n","    # Convolution 32 filters\n","    cumulative_resulting_tensor = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),\n","                                                         strides=(1, 1),\n","                                                         padding='same',\n","                                                         activation='linear',\n","                                                         kernel_regularizer=tf.keras.regularizers.l2(\n","                                                             L2_REG_LAMBDA),\n","                                                         bias_regularizer=tf.keras.regularizers.l2(L2_REG_LAMBDA))(cumulative_resulting_tensor)\n","    # RELU\n","    cumulative_resulting_tensor = tf.keras.layers.Activation(\n","        'relu')(cumulative_resulting_tensor)\n","    # Convolution 32 filters\n","    cumulative_resulting_tensor = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),\n","                                                         strides=(1, 1),\n","                                                         padding='same',\n","                                                         activation='linear',\n","                                                         kernel_regularizer=tf.keras.regularizers.l2(\n","                                                             L2_REG_LAMBDA),\n","                                                         bias_regularizer=tf.keras.regularizers.l2(L2_REG_LAMBDA))(cumulative_resulting_tensor)\n","    # RELU\n","    cumulative_resulting_tensor = tf.keras.layers.Activation(\n","        'relu')(cumulative_resulting_tensor)\n","\n","\n","# Fourth processing layer\n","if True:\n","    # Deconvolution 32 filters\n","    cumulative_resulting_tensor = tf.keras.layers.Conv2DTranspose(32, kernel_size=(2, 2),\n","                                                                  strides=(\n","                                                                      2, 2),\n","                                                                  padding='same',\n","                                                                  activation='linear',\n","                                                                  kernel_regularizer=tf.keras.regularizers.l2(\n","                                                                      L2_REG_LAMBDA),\n","                                                                  bias_regularizer=tf.keras.regularizers.l2(L2_REG_LAMBDA))(cumulative_resulting_tensor)\n","    # RELU\n","    cumulative_resulting_tensor = tf.keras.layers.Activation(\n","        'relu')(cumulative_resulting_tensor)\n","    # Concatenation\n","    cumulative_resulting_tensor = tf.keras.layers.Concatenate(\n","        axis=CONCATENATION_DIRECTION_OF_FEATURES)([intermediate_tensor_2, cumulative_resulting_tensor])\n","    # Convolution 32 filters\n","    cumulative_resulting_tensor = tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n","                                                         strides=(1, 1),\n","                                                         padding='same',\n","                                                         activation='linear',\n","                                                         kernel_regularizer=tf.keras.regularizers.l2(\n","                                                             L2_REG_LAMBDA),\n","                                                         bias_regularizer=tf.keras.regularizers.l2(L2_REG_LAMBDA))(cumulative_resulting_tensor)\n","    # RELU\n","    cumulative_resulting_tensor = tf.keras.layers.Activation(\n","        'relu')(cumulative_resulting_tensor)\n","    # Convolution 32 filters\n","    cumulative_resulting_tensor = tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n","                                                         strides=(1, 1),\n","                                                         padding='same',\n","                                                         activation='linear',\n","                                                         kernel_regularizer=tf.keras.regularizers.l2(\n","                                                             L2_REG_LAMBDA),\n","                                                         bias_regularizer=tf.keras.regularizers.l2(L2_REG_LAMBDA))(cumulative_resulting_tensor)\n","    # RELU\n","    cumulative_resulting_tensor = tf.keras.layers.Activation(\n","        'relu')(cumulative_resulting_tensor)\n","\n","# Fifth processing layer\n","if True:\n","    # Deconvolution 16 filters\n","    cumulative_resulting_tensor = tf.keras.layers.Conv2DTranspose(16, kernel_size=(2, 2),\n","                                                                  strides=(\n","                                                                      2, 2),\n","                                                                  padding='same',\n","                                                                  activation='linear',\n","                                                                  kernel_regularizer=tf.keras.regularizers.l2(\n","                                                                      L2_REG_LAMBDA),\n","                                                                  bias_regularizer=tf.keras.regularizers.l2(L2_REG_LAMBDA))(cumulative_resulting_tensor)\n","    # RELU\n","    cumulative_resulting_tensor = tf.keras.layers.Activation(\n","        'relu')(cumulative_resulting_tensor)\n","    # Concatenation\n","    cumulative_resulting_tensor = tf.keras.layers.Concatenate(\n","        axis=CONCATENATION_DIRECTION_OF_FEATURES)([intermediate_tensor_1, cumulative_resulting_tensor])\n","    # Convolution 16 filters\n","    cumulative_resulting_tensor = tf.keras.layers.Conv2D(16, kernel_size=(3, 3),\n","                                                         strides=(1, 1),\n","                                                         padding='same',\n","                                                         activation='linear',\n","                                                         kernel_regularizer=tf.keras.regularizers.l2(\n","                                                             L2_REG_LAMBDA),\n","                                                         bias_regularizer=tf.keras.regularizers.l2(L2_REG_LAMBDA))(cumulative_resulting_tensor)\n","    # RELU\n","    cumulative_resulting_tensor = tf.keras.layers.Activation(\n","        'relu')(cumulative_resulting_tensor)\n","    # Convolution 16 filters\n","    cumulative_resulting_tensor = tf.keras.layers.Conv2D(4, kernel_size=(3, 3),\n","                                                         strides=(1, 1),\n","                                                         padding='same',\n","                                                         activation='linear',\n","                                                         kernel_regularizer=tf.keras.regularizers.l2(\n","                                                             L2_REG_LAMBDA),\n","                                                         bias_regularizer=tf.keras.regularizers.l2(L2_REG_LAMBDA))(cumulative_resulting_tensor)\n","    # RELU\n","    cumulative_resulting_tensor = tf.keras.layers.Activation(\n","        'relu')(cumulative_resulting_tensor)\n","\n","# Sixth processing layer\n","if True:\n","    # Convolution 1 filter sigmoidal (to make size converge to final one)\n","    cumulative_resulting_tensor = tf.keras.layers.Conv2D(N_CLASSES, kernel_size=(1, 1),\n","                                                         strides=(1, 1),\n","                                                         padding='same',\n","                                                         activation='linear',\n","                                                         kernel_regularizer=tf.keras.regularizers.l2(\n","                                                             L2_REG_LAMBDA),\n","                                                         bias_regularizer=tf.keras.regularizers.l2(L2_REG_LAMBDA))(cumulative_resulting_tensor)\n","\n","    my_output_tensor = tf.keras.layers.Softmax(\n","        axis=-1)(cumulative_resulting_tensor)\n","\n","my_model = tf.keras.Model(inputs=[my_input_tensor], outputs=[my_output_tensor])\n","print('Parameters in model: {}'.format(my_model.count_params()))"]},{"cell_type":"markdown","metadata":{"id":"4Grzd6h5fiCM"},"source":["## Model training\n","Since it may happen that the training process of our model stops (due to disconnection from Google Colab VM or other problems) we first check if we need to recover the training process. This is done by checking the existance of a file called \"Running\" that is deleted once the training is completed. If such a file exists, the model recovers the training process, otherwise a new training is started.\n","\n","---\n","\n","First, we define some useful functions that we will use to recover previous weights."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2OXHtKbjfkNb"},"outputs":[],"source":["def find_latest_data_folder(path):\n","    '''\n","    This function finds the latest data folder among all the data folders.\n","    It does so by looking at the folder name, that contains information\n","    about year, month, day and time. By checking the time-delta\n","    from each folder to the current time, it is possible to find\n","    the latest one.\n","    '''\n","    data_folders = os.listdir(path)  # Get list of data folders\n","    data_folders_dt = []  # Init datetime list of data folder names\n","    data_folders_timedelta = []  # Init timedelta list\n","    for folder in data_folders:\n","        try:\n","            # We need two lists, otherwise the indexes won't match\n","            data_folders_timedelta.append(\n","                datetime.now() - datetime.strptime(folder, '%Y%m%d_%H%M%S'))\n","            data_folders_dt.append(folder)\n","        except ValueError:\n","            continue\n","    if (len(data_folders_dt) > 0):\n","        # Find latest data folder based on timedelta\n","        latest_data_folder = data_folders_dt[np.argmin(data_folders_timedelta)]\n","        # Return full path\n","        data_folder = os.path.join(path, latest_data_folder)\n","    else:\n","        data_folder = ''\n","    return data_folder\n","\n","\n","def find_latest_weights_file(path):\n","    '''\n","    This function finds the latest data folder among all the data folders.\n","    It does so by looking at the folder name, that contains information\n","    about year, month, day and time. By checking the time-delta\n","    from each folder to the current time, it is possible to find\n","    the latest one.\n","    '''\n","    file_names = os.listdir(path)  # Get list of data folders\n","    highest_epoch_file_name = ''\n","    max_epoch = -1\n","    for file_name in file_names:\n","        if ('weights' in file_name):\n","            if (file_name[10] == '-'):\n","                if (int(file_name[8:10]) > max_epoch):\n","                    max_epoch = int(file_name[8:10])\n","                    highest_epoch_file_name = file_name\n","            else:\n","                if (int(file_name[8:11]) > max_epoch):\n","                    max_epoch = int(file_name[8:11])\n","                    highest_epoch_file_name = file_name\n","    return highest_epoch_file_name, max_epoch"]},{"cell_type":"markdown","metadata":{"id":"0sVb3NWlflgk"},"source":["Then, we check if the training of a model started. If so, we recover the weights. Otherwise we start a new training process."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"elapsed":1467,"status":"ok","timestamp":1595322298010,"user":{"displayName":"Neuro Engineer","photoUrl":"","userId":"17805442247735853420"},"user_tz":-120},"id":"hmK0uH0nfl1I","outputId":"dd378dc1-4ee2-4d09-bbfb-a706a1930284"},"outputs":[{"name":"stdout","output_type":"stream","text":["No previous model found. Starting training from scratch.\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Starting model from epoch: 0\n"]}],"source":["# Check if we have already started training a model, but the training\n","# could not be completed\n","weights_found = False\n","if (os.path.exists(os.path.join(TEMP_PATH, 'running'))):\n","    print('Running file found in folder.')\n","    # Reload weights\n","    data_folder = find_latest_data_folder(TEMP_PATH)\n","    if (not (data_folder == '')):\n","        model_weights_folder = os.path.join(\n","            data_folder, '{}_{}'.format(N_ROWS, N_COLUMNS))\n","        weights_file, last_epoch = find_latest_weights_file(\n","            model_weights_folder)\n","        if ('weights' in weights_file):\n","            weights_found = True\n","            print('Loading weights from: {}'.format(\n","                os.path.join(model_weights_folder, weights_file)))\n","            my_model.load_weights(os.path.join(\n","                model_weights_folder, weights_file))\n","            plotMetrics.load_data(data_folder)\n","\n","if (not weights_found):\n","    print('No previous model found. Starting training from scratch.')\n","    if (os.path.exists(os.path.join(TEMP_PATH, 'running'))):\n","        os.remove(os.path.join(TEMP_PATH, 'running'))\n","    # We need to train the model from scratch\n","    data_folder = os.path.join(TEMP_PATH,\n","                               datetime.strftime(datetime.now(), \"%Y%m%d_%H%M%S\"))\n","    model_weights_folder = os.path.join(data_folder,\n","                                        '{}_{}'.format(N_ROWS, N_COLUMNS)\n","                                        )\n","    if (not (os.path.exists(model_weights_folder))):\n","        os.makedirs(model_weights_folder)\n","    last_epoch = 0\n","\n","# Set folder where to store training monitoring plot\n","plotMetrics.set_folder(data_folder)\n","# Set checkpoint to save weights\n","model_checkpoint_filepath = os.path.join(model_weights_folder,\n","                                         'weights.{epoch:03d}-{val_loss:.2f}.hdf5'\n","                                         )\n","model_checkpoint_save = tf.keras.callbacks.ModelCheckpoint(filepath=model_checkpoint_filepath,\n","                                                           save_weights_only=True,\n","                                                           monitor='val_loss',\n","                                                           period=5)\n","my_callbacks_list.append(model_checkpoint_save)\n","print('Starting model from epoch: {}'.format(last_epoch))"]},{"cell_type":"markdown","metadata":{"id":"JG_1-D_lft9A"},"source":["As a last step, we compile and fit the model. Once the training of the model is complete, we delete the file \"Running\" and we print out the total time required for the model to complete the training process.\n","\n","The parameters that can be set in the generator init function and in the fit generator function are:\n","- **batch_size**: determines the number of samples in each mini batch. Its maximum is the number of all samples, which makes gradient descent accurate, the loss will decrease towards the minimum if the learning rate is small enough, but iterations are slower. Its minimum is 1, resulting in stochastic gradient descent: Fast but the direction of the gradient step is based only on one example, the loss may jump around. batch_size allows to adjust between the two extremes: accurate gradient direction and fast iteration. Also, the maximum value for batch_size may be limited if your model + data set does not fit into the available (GPU) memory.\n","- **steps_per_epoch**: the number of batch iterations before a training epoch is considered finished. If you have a training set of fixed size you can ignore it but it may be useful if you have a huge data set or if you are generating random data augmentations on the fly, i.e. if your training set has a (generated) infinite size. If you have the time to go through your whole training data set I recommend to skip this parameter.\n","- **validation_steps**: similar to steps_per_epoch but on the validation data set instead on the training data. If you have the time to go through your whole validation data set I recommend to skip this parameter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xfx1cv_cftPa"},"outputs":[],"source":["# Compile model\n","my_model.compile(\n","    # optimizer = keras.optimizers.Adam(learning_rate = my_learning_rate_program()), # arg = 0 by default\n","    optimizer=keras.optimizers.Adam(learning_rate=3e-5),  # arg = 0 by default\n","    loss=tf.keras.losses.CategoricalCrossentropy(),\n","    metrics=my_metrics\n",")\n","\n","# Get list of file names for volumes and labels\n","train_array = [list(data)\n","               for data in zip(train_volumes_list, train_labels_list)]\n","validation_array = [list(data) for data in zip(\n","    validation_volumes_list, validation_labels_list)]\n","\n","# Set up train and validation generator\n","train_generator = DataGenerator(\n","    train_array, batch_size=100, dim=(N_ROWS, N_COLUMNS), n_slices=N_SLICES)\n","validation_generator = DataGenerator(\n","    validation_array, batch_size=100, dim=(N_ROWS, N_COLUMNS), n_slices=N_SLICES)\n","\n","# Get current time\n","tic = datetime.now()\n","\n","if (not (os.path.exists(os.path.join(TEMP_PATH)))):\n","    os.makedirs(TEMP_PATH)\n","\n","# Write file so that we can recover the last weights\n","if (not (os.path.exists(os.path.join(TEMP_PATH, 'running')))):\n","    with open(os.path.join(TEMP_PATH, 'running'), 'w') as f:\n","        f.write('Running')\n","\n","# Fit model\n","monitoring = my_model.fit(train_generator,\n","                          steps_per_epoch=math.ceil(\n","                              len(train_array)*N_SLICES / BATCH_SIZE),\n","                          epochs=MAX_EPOCHS,\n","                          validation_data=validation_generator,\n","                          validation_steps=math.ceil(\n","                              len(validation_array)*N_SLICES / 10),\n","                          callbacks=my_callbacks_list,\n","                          max_queue_size=1,\n","                          workers=4,\n","                          initial_epoch=last_epoch)\n","\n","# Print out total time\n","toc = datetime.now()\n","seconds_training = (toc-tic).total_seconds()\n","hours, remainder = divmod(seconds_training, 3600)\n","minutes, seconds = divmod(remainder, 60)\n","print('\\n\\nTraining completed!!\\n{:02d}:{:02d}:{:02d} spent for training'.format(\n","    int(hours), int(minutes), int(seconds)))\n","\n","# Delete file \"Running\"\n","if (os.path.exists(os.path.join(TEMP_PATH, 'runnning'))):\n","    os.remove(os.path.join(TEMP_PATH, 'running'))\n","\n","# Save the model\n","owd = os.getcwd()\n","if (not (os.path.exists(MODEL_PATH))):\n","    os.makedirs(MODEL_PATH)\n","os.chdir(MODEL_PATH)\n","number_of_models = len(os.listdir())\n","my_model.save('model_{}_{}x{}x{}.h5'.format(\n","    number_of_models, N_ROWS, N_COLUMNS, N_SLICES))\n","os.chdir(owd)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
